{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd754d1-0594-4c12-8fa9-8dc5817a0b1c",
   "metadata": {},
   "source": [
    "# SFO AI Hackathon - ReCaptcha Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cef99c9-48ee-401e-93fe-dfe16bcda819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c812119-6d29-430b-9e3a-2919ed7ba7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcec1c6-73b1-4475-9c3b-e3e0679c408a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469eac54-b5f3-400e-ac8f-9e4f4300063f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0880c1-6bb0-40d0-9c42-287265a7d0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ImageCaptionLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f5f27c-cdc8-4f66-a56e-ab5786c231d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images found: 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "files = os.listdir(current_dir)\n",
    "\n",
    "# Filter out any non-image files\n",
    "images = []\n",
    "for file in files:\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n",
    "        images.append(file)\n",
    "\n",
    "print(\"Images found:\", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994aad94-49f0-4627-a361-ae183de72214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_image_0_2.jpg',\n",
       " 'output_image_2_1.jpg',\n",
       " 'output_image_0_1.jpg',\n",
       " 'output_image_1_0.jpg',\n",
       " 'output_image_2_0.jpg',\n",
       " 'output_image_1_1.jpg',\n",
       " 'output_image_2_2.jpg',\n",
       " 'output_image_1_2.jpg',\n",
       " 'output_image_0_0.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f46832-20db-40a7-8e54-c8e113fc162e",
   "metadata": {},
   "source": [
    "## Image Captioning using LangChain Image captions\n",
    "# Salesforce BLIP image captioning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae00b3d-7d23-4a08-a299-ef5d80ddbd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='an image of a person riding a motorcycle [SEP]', metadata={'image_path': 'output_image_0_2.jpg'}),\n",
       " Document(page_content='an image of a street in the middle of a town [SEP]', metadata={'image_path': 'output_image_2_1.jpg'}),\n",
       " Document(page_content='an image of a house with a driveway [SEP]', metadata={'image_path': 'output_image_0_1.jpg'}),\n",
       " Document(page_content='an image of a building with a blue door [SEP]', metadata={'image_path': 'output_image_1_0.jpg'}),\n",
       " Document(page_content='an image of a building with a car parked in front [SEP]', metadata={'image_path': 'output_image_2_0.jpg'}),\n",
       " Document(page_content='an image of a street with cars parked on it [SEP]', metadata={'image_path': 'output_image_1_1.jpg'}),\n",
       " Document(page_content='an image of a parking lot with cars parked in it [SEP]', metadata={'image_path': 'output_image_2_2.jpg'}),\n",
       " Document(page_content='an image of a person riding a motorcycle [SEP]', metadata={'image_path': 'output_image_1_2.jpg'}),\n",
       " Document(page_content='an image of a motorcycle parked in front of a building [SEP]', metadata={'image_path': 'output_image_0_0.jpg'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = ImageCaptionLoader(images=images)\n",
    "list_docs = loader.load()\n",
    "list_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103d0ea-9cd8-4fb6-9955-65ce755d8080",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Amazon Bedrock LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a49d8a-7904-4f4e-9ee2-2ee74c9c5e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Amazon Bedrock - boto3\n",
    "import boto3\n",
    "\n",
    "# Setup bedrock\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "# LLM - Amazon Bedrock LLM using LangChain\n",
    "from langchain.llms import Bedrock\n",
    "model_id = \"anthropic.claude-v2\"\n",
    "model_kwargs =  { \n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\n",
    "      \"\\n\\nHuman:\"\n",
    "    ],\n",
    "}\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "# Embeddings Model - Amazon Titan Embeddings Model using LangChain\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "# create embeddings\n",
    "bedrock_embedding = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20756f27-c26e-43b5-b24f-19e9bc7142f1",
   "metadata": {},
   "source": [
    "## Print image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68656a33-a8e4-45be-8694-ba1d5492e820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image of a person riding a motorcycle [SEP]\n",
      "{'image_path': 'output_image_0_2.jpg'}\n",
      "an image of a street in the middle of a town [SEP]\n",
      "{'image_path': 'output_image_2_1.jpg'}\n",
      "an image of a house with a driveway [SEP]\n",
      "{'image_path': 'output_image_0_1.jpg'}\n",
      "an image of a building with a blue door [SEP]\n",
      "{'image_path': 'output_image_1_0.jpg'}\n",
      "an image of a building with a car parked in front [SEP]\n",
      "{'image_path': 'output_image_2_0.jpg'}\n",
      "an image of a street with cars parked on it [SEP]\n",
      "{'image_path': 'output_image_1_1.jpg'}\n",
      "an image of a parking lot with cars parked in it [SEP]\n",
      "{'image_path': 'output_image_2_2.jpg'}\n",
      "an image of a person riding a motorcycle [SEP]\n",
      "{'image_path': 'output_image_1_2.jpg'}\n",
      "an image of a motorcycle parked in front of a building [SEP]\n",
      "{'image_path': 'output_image_0_0.jpg'}\n"
     ]
    }
   ],
   "source": [
    "for item in list_docs:\n",
    "    print(item.page_content)\n",
    "    print(item.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846656e-00f3-4738-8d6e-12c6889a4bde",
   "metadata": {},
   "source": [
    "## LangChain VectorStore using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38b8f92b-ac3a-4de8-8e15-ab15d90954d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "documents=list_docs,\n",
    "embedding=bedrock_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae9255-1b8c-4b5f-99b1-1d024ed9743f",
   "metadata": {},
   "source": [
    "## Similarity Search with Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56dafd92-809e-43e6-b791-ddf82b339cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"car\"\n",
    "docs = vectorstore.similarity_search_with_score(question, k=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95cbe4a7-6624-4404-8004-ec89c08c5555",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='an image of a building with a car parked in front [SEP]', metadata={'image_path': 'output_image_2_0.jpg'}),\n",
       "  476.55633544921875),\n",
       " (Document(page_content='an image of a street with cars parked on it [SEP]', metadata={'image_path': 'output_image_1_1.jpg'}),\n",
       "  477.7770080566406),\n",
       " (Document(page_content='an image of a parking lot with cars parked in it [SEP]', metadata={'image_path': 'output_image_2_2.jpg'}),\n",
       "  522.1278076171875),\n",
       " (Document(page_content='an image of a house with a driveway [SEP]', metadata={'image_path': 'output_image_0_1.jpg'}),\n",
       "  528.5362548828125),\n",
       " (Document(page_content='an image of a motorcycle parked in front of a building [SEP]', metadata={'image_path': 'output_image_0_0.jpg'}),\n",
       "  530.2296752929688),\n",
       " (Document(page_content='an image of a person riding a motorcycle [SEP]', metadata={'image_path': 'output_image_0_2.jpg'}),\n",
       "  546.64599609375),\n",
       " (Document(page_content='an image of a person riding a motorcycle [SEP]', metadata={'image_path': 'output_image_1_2.jpg'}),\n",
       "  546.64599609375),\n",
       " (Document(page_content='an image of a street in the middle of a town [SEP]', metadata={'image_path': 'output_image_2_1.jpg'}),\n",
       "  614.9840698242188),\n",
       " (Document(page_content='an image of a building with a blue door [SEP]', metadata={'image_path': 'output_image_1_0.jpg'}),\n",
       "  681.0945434570312)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ecfe3a-f8d4-432b-b86b-04d9c0d2366a",
   "metadata": {},
   "source": [
    "## Using LLM (Claude V2) to validate Similarity Search output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ae5f0c-301a-40cd-882f-d367a390f43a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are the results in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"image_path\": \"output_image_2_0.jpg\", \n",
      "    \"match\": \"Yes\"\n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_1_1.jpg\",\n",
      "    \"match\": \"Yes\"  \n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_2_2.jpg\",\n",
      "    \"match\": \"Yes\"\n",
      "  },\n",
      "  {  \n",
      "    \"image_path\": \"output_image_0_1.jpg\",\n",
      "    \"match\": \"No\"\n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_0_0.jpg\", \n",
      "    \"match\": \"No\"\n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_0_2.jpg\",\n",
      "    \"match\": \"No\"\n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_1_2.jpg\", \n",
      "    \"match\": \"No\"\n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_2_1.jpg\",\n",
      "    \"match\": \"No\"\n",
      "  },\n",
      "  {\n",
      "    \"image_path\": \"output_image_1_0.jpg\",\n",
      "    \"match\": \"No\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "question = \"car\"\n",
    "\n",
    "query = f\"\"\"\n",
    "\\n\\nHuman:\n",
    "You are a Capcha expert.\n",
    "\n",
    "Review the following context.\n",
    "<context>\n",
    "{docs}\n",
    "</context>\n",
    "\n",
    "Iterate each lines and provide output using the following template in json format:\n",
    "\n",
    "- image_path: image_path , match: say Yes when data contains {question}, if not say No. \n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "response = llm.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106fc30-118d-4f4f-b28d-560e5942583a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
